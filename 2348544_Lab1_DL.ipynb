{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOnySP8UEkyHhpvGMJMSaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-727/DL-Lab-work/blob/main/2348544_Lab1_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_jOxWC_2CnA5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, n_iterations=100,initial_weights=None):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        if initial_weights is not None and len(initial_weights) >= 3:\n",
        "            self.weights = np.array(initial_weights[:2])\n",
        "            self.bias = initial_weights[2]\n",
        "        else:\n",
        "            self.weights = None\n",
        "            self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        if self.weights is None:\n",
        "            self.weights = np.zeros(n_features)\n",
        "        if self.bias is None:\n",
        "            self.bias = 0\n",
        "\n",
        "        errors = []\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            total_error = 0\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.activation_function(linear_output)\n",
        "                error = y[idx] - y_predicted\n",
        "\n",
        "                update = self.learning_rate * error\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "                total_error += error**2\n",
        "            errors.append(total_error)\n",
        "\n",
        "        return errors\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.activation_function(linear_output)\n",
        "        return y_predicted\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "and_outputs = np.array([0, 0, 0, 1])\n",
        "or_outputs = np.array([0, 1, 1, 1])\n",
        "nand_outputs = np.array([1, 1, 1, 0])\n",
        "\n",
        "initial_weights = [0.3, -0.1, 0.2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(name, outputs):\n",
        "    perceptron = Perceptron(initial_weights=initial_weights)\n",
        "    errors = perceptron.fit(inputs, outputs)\n",
        "    print(f\"{name} Predictions after training:\", perceptron.predict(inputs))\n",
        "    print(f\"{name} Errors over iterations:\", errors[-1])\n",
        "\n",
        "    print(f\"{name} Final Weights:\", perceptron.weights)\n",
        "    print(f\"{name} Final Bias:\", perceptron.bias)"
      ],
      "metadata": {
        "id": "nfZuNluCFebT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#And function evaluation"
      ],
      "metadata": {
        "id": "RGFzHMiGFwJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AND Function:\")\n",
        "train_and_evaluate(\"AND\", and_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJlOitfeFhz5",
        "outputId": "47aca0ea-7590-42e5-cf40-a7100ce6e936"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Function:\n",
            "AND Predictions after training: [0 0 0 1]\n",
            "AND Errors over iterations: 0\n",
            "AND Final Weights: [0.2 0.2]\n",
            "AND Final Bias: -0.30000000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OR function evaluation"
      ],
      "metadata": {
        "id": "4DypPNd0F0MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOR Function:\")\n",
        "train_and_evaluate(\"OR\", or_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9qQx06TFjuU",
        "outputId": "f762cc41-a7b4-46cd-c1dc-6f564df92029"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OR Function:\n",
            "OR Predictions after training: [0 1 1 1]\n",
            "OR Errors over iterations: 0\n",
            "OR Final Weights: [0.3 0.1]\n",
            "OR Final Bias: -0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Nand function evaluation"
      ],
      "metadata": {
        "id": "CltJf9cQF2EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNAND Function:\")\n",
        "train_and_evaluate(\"NAND\", nand_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzDc-zqoFmEf",
        "outputId": "d1499dd9-f760-4c0c-c5ee-04a9a7152ff9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NAND Function:\n",
            "NAND Predictions after training: [1 1 1 0]\n",
            "NAND Errors over iterations: 0\n",
            "NAND Final Weights: [-0.1 -0.1]\n",
            "NAND Final Bias: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The XOR function presents a challenge for a single-layer perceptron due to its non-linear separability. A single-layer perceptron cannot solve problems that are not linearly separable, which is the case for XOR.\n",
        "perceptron won't work for XOR. Solving XOR would require a multi-layer perceptron with at least one hidden layer, which involves concepts beyond the scope of the basic perceptron learning algorithm."
      ],
      "metadata": {
        "id": "LsRgyptnDLY-"
      }
    }
  ]
}